# AI-Security
This Repository Contains all the resources to learn about AI Security.

If you want to contribute, create a PR or contact me [@Green_terminals](https://twitter.com/Green_terminals).

## OWASP Top 10 for Large Language Model Applications
- https://owasp.org/www-project-top-10-for-large-language-model-applications/

## Web LLM attacks
- https://portswigger.net/web-security/llm-attacks

## Articles

- https://josephthacker.com/ai/2023/08/25/prompt-injection-primer.html
- https://research.nccgroup.com/2022/12/05/exploring-prompt-injection-attacks/
- https://infosecwriteups.com/art-of-hacking-llm-apps-a22cf60a523b
- https://infosecwriteups.com/bypassing-kyc-using-deepfake-e11f0722c722
- https://learnprompting.org/docs/prompt_hacking/injection
- https://boringappsec.substack.com/p/guest-post-edition-24-pentesting

## Videos

- [Attacking LLM - Prompt Injection](https://www.youtube.com/watch?v=Sv5OLj2nVAQ)
- [Accidental LLM Backdoor - Prompt Tricks](https://www.youtube.com/watch?v=h74oXb4Kk8k)
- [Defending LLM - Prompt Injection](https://www.youtube.com/watch?v=VbNPZ1n6_vY)
- [Prompt Injection 101 - Understanding Security Risks in LLM](https://www.youtube.com/watch?v=TDS6PGfniIU)
- [AI Hacking üî• OWASP Top 10 Vulnerabilities in LLM Applications](https://www.youtube.com/watch?v=engR9tYSsug)
- [Prompt Injection üéØ AI hacking & LLM Attacks](https://www.youtube.com/watch?v=86AFddhX2zc)
- [Intro to AI Security](https://www.youtube.com/playlist?list=PLIk64WCt6NAd5ReSGLwXnzCX6HKgVAHjy)
- [Daniel Miessler and Rez0: Hacking with AI (Ep. 24)](https://www.youtube.com/watch?v=Jt2d3XA07ig)
- [AI and hacking - opportunities and threats - Joseph ‚Äúrez0‚Äù Thacker](https://www.youtube.com/watch?v=zY7dz4Dx5tc)
- [AI Application Security by Joseph Thacker](https://www.youtube.com/watch?v=Jj_GMUBpolc)
- [Invisible Prompt Injection Explained](https://www.youtube.com/watch?v=6IYi7pqGRoU)
- [Hacking into Pretrained ML model by S.G Harish](https://www.youtube.com/watch?v=QiZg4JSbDgQ)

## CTFs

- [GPT Prompt Attack](https://gpa.43z.one/)
- [Double Speak Chat](https://doublespeak.chat/)
- [Gandalf](https://gandalf.lakera.ai/)
- [DamnVulnerableLLMProject](https://github.com/harishsg993010/DamnVulnerableLLMProject)

## Research Papers

- [Extracting Training Data from Large Language Models](https://arxiv.org/pdf/2012.07805.pdf)
- [Poisoning Language Models During Instruction Tuning](https://arxiv.org/pdf/2305.00944.pdf)
- [Stealing Machine Learning Models via Prediction APIs](https://arxiv.org/abs/1609.02943)
- [Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models](https://arxiv.org/abs/2305.14710)
- [Prompt Injection attack against LLM-integrated Applications](https://arxiv.org/abs/2306.05499)
- [Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks](https://arxiv.org/abs/2302.05733)
- [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://llm-attacks.org/)

## Adversarial Machine Learning

- https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-2e2023.pdf

## Tools

- [Garak](https://github.com/leondz/garak)
- [LLMFuzzer](https://github.com/mnns/LLMFuzzer)

## AI Attack Surface Map

- [The AI Attack Surface Map v1.0](https://danielmiessler.com/p/the-ai-attack-surface-map-v1-0/)

## Twitter Accounts To Follow

- [Joseph Thacker](https://twitter.com/rez0__)
- [LLM Security](https://twitter.com/llm_sec)
- [Riley Goodside](https://twitter.com/goodside)
- [·¥Ö·¥Ä…¥…™·¥á ü ·¥ç…™·¥áss ü·¥á Ä](https://twitter.com/DanielMiessler)





